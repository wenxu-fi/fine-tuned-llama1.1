{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T14:19:24.101669Z","iopub.execute_input":"2025-01-31T14:19:24.101875Z","iopub.status.idle":"2025-01-31T14:19:24.549167Z","shell.execute_reply.started":"2025-01-31T14:19:24.101855Z","shell.execute_reply":"2025-01-31T14:19:24.548279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Install Required Libraries}$$","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:56:54.124806Z","iopub.execute_input":"2025-02-19T07:56:54.125025Z","iopub.status.idle":"2025-02-19T07:57:19.381256Z","shell.execute_reply.started":"2025-02-19T07:56:54.125004Z","shell.execute_reply":"2025-02-19T07:57:19.380367Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nydata-profiling 4.12.1 requires scipy<1.14,>=1.4.1, but you have scipy 1.15.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -U \"scikit-learn==1.5.2\" \"matplotlib==3.9.4\" \"scipy==1.13.1\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:10:23.803182Z","iopub.execute_input":"2025-02-19T12:10:23.803488Z","iopub.status.idle":"2025-02-19T12:10:44.981347Z","shell.execute_reply.started":"2025-02-19T12:10:23.803441Z","shell.execute_reply":"2025-02-19T12:10:44.980509Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.5.2\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting matplotlib==3.9.4\n  Downloading matplotlib-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.26.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.2) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (24.2)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.4) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.5->scikit-learn==1.5.2) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.4) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.5->scikit-learn==1.5.2) (2024.2.0)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading matplotlib-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, matplotlib\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\nSuccessfully installed matplotlib-3.9.4 scikit-learn-1.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:11:57.675047Z","iopub.execute_input":"2025-02-19T12:11:57.675352Z","iopub.status.idle":"2025-02-19T12:12:15.202746Z","shell.execute_reply.started":"2025-02-19T12:11:57.675326Z","shell.execute_reply":"2025-02-19T12:12:15.201673Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\nprint(\"Rouge Score version:\", rouge_score.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:13:34.235192Z","iopub.execute_input":"2025-02-19T12:13:34.235600Z","iopub.status.idle":"2025-02-19T12:13:54.785606Z","shell.execute_reply.started":"2025-02-19T12:13:34.235566Z","shell.execute_reply":"2025-02-19T12:13:54.784478Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.49.0\nPEFT version: 0.14.0\nAccelerate version: 1.4.0\nDatasets version: 3.3.1\nScipy version: 1.13.1\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c763d708db99>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluate version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRL version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rouge Score version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'rouge_score' has no attribute '__version__'"],"ename":"AttributeError","evalue":"module 'rouge_score' has no attribute '__version__'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import rouge_score\nfrom importlib.metadata import version\n\nprint(\"Rouge Score version:\", version(\"rouge_score\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:14:07.393562Z","iopub.execute_input":"2025-02-19T12:14:07.393887Z","iopub.status.idle":"2025-02-19T12:14:07.399505Z","shell.execute_reply.started":"2025-02-19T12:14:07.393863Z","shell.execute_reply":"2025-02-19T12:14:07.398799Z"}},"outputs":[{"name":"stdout","text":"Rouge Score version: 0.1.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Load the Dataset from hugging face() }$$","metadata":{}},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:16:27.859157Z","iopub.execute_input":"2025-02-19T12:16:27.859555Z","iopub.status.idle":"2025-02-19T12:16:27.863166Z","shell.execute_reply.started":"2025-02-19T12:16:27.859524Z","shell.execute_reply":"2025-02-19T12:16:27.862360Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:16:46.827712Z","iopub.execute_input":"2025-02-19T12:16:46.827996Z","iopub.status.idle":"2025-02-19T12:17:13.228835Z","shell.execute_reply.started":"2025-02-19T12:16:46.827976Z","shell.execute_reply":"2025-02-19T12:17:13.227924Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":6},{"cell_type":"markdown","source":"In above cell,\"from huggingface_hub import interpreter_login\" should be modifeied to \"from huggingface_hub import login\n\" \n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:14:09.157251Z","iopub.execute_input":"2025-02-19T08:14:09.157655Z","iopub.status.idle":"2025-02-19T08:14:09.178310Z","shell.execute_reply.started":"2025-02-19T08:14:09.157622Z","shell.execute_reply":"2025-02-19T08:14:09.177186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb08f83ad79e4680a603a2dbc4cfe2df"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:17:41.104516Z","iopub.execute_input":"2025-02-19T12:17:41.104819Z","iopub.status.idle":"2025-02-19T12:17:41.109213Z","shell.execute_reply.started":"2025-02-19T12:17:41.104798Z","shell.execute_reply":"2025-02-19T12:17:41.108296Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:17:48.612990Z","iopub.execute_input":"2025-02-19T12:17:48.613317Z","iopub.status.idle":"2025-02-19T12:17:51.787632Z","shell.execute_reply.started":"2025-02-19T12:17:48.613290Z","shell.execute_reply":"2025-02-19T12:17:51.786787Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99370ebe48fc479ba35f38313b15483c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"382ab0645d954be895da24aaac41ef73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf270f8e7894d968972d7197118ffc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7699556ef75404aa31ea3475b7f6b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa1b3fe32ed84faab5df5b2980312b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d299aeef8bf94558b4a03684a0adb85c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6ea6a46374418ba39e40a66a1cde21"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:18:03.961573Z","iopub.execute_input":"2025-02-19T12:18:03.961868Z","iopub.status.idle":"2025-02-19T12:18:03.968697Z","shell.execute_reply.started":"2025-02-19T12:18:03.961847Z","shell.execute_reply":"2025-02-19T12:18:03.967855Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Create Bitsandbytes Configuration }$$","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:18:08.467164Z","iopub.execute_input":"2025-02-19T12:18:08.467525Z","iopub.status.idle":"2025-02-19T12:18:08.473618Z","shell.execute_reply.started":"2025-02-19T12:18:08.467491Z","shell.execute_reply":"2025-02-19T12:18:08.472761Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Load the Pre-Trained Model }$$","metadata":{}},{"cell_type":"code","source":"model_name='maneln/llama-1.1b'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:18:14.819676Z","iopub.execute_input":"2025-02-19T12:18:14.819990Z","iopub.status.idle":"2025-02-19T12:20:02.914054Z","shell.execute_reply.started":"2025-02-19T12:18:14.819963Z","shell.execute_reply":"2025-02-19T12:20:02.913215Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7775c6d1f624747a82e544e878ba4dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09685105db8e435986ad1488c6469712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0c48293db84e96ba54a4129f8354e4"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Tokenization}$$","metadata":{}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:20:47.219886Z","iopub.execute_input":"2025-02-19T12:20:47.220220Z","iopub.status.idle":"2025-02-19T12:20:48.836267Z","shell.execute_reply.started":"2025-02-19T12:20:47.220189Z","shell.execute_reply":"2025-02-19T12:20:48.835616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8870ce4eb244f0aae68bce809ebd6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703066b6cab444d9b76efe425eba3f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95eadc36db5c48a18b4eb9ec3fddb7af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21b57ffb79684e598463a280ef1e243f"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:23:09.345032Z","iopub.execute_input":"2025-02-19T12:23:09.345406Z","iopub.status.idle":"2025-02-19T12:23:09.350548Z","shell.execute_reply.started":"2025-02-19T12:23:09.345374Z","shell.execute_reply":"2025-02-19T12:23:09.349680Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1224 MB.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Test the Model with Zero-Shot Inference}$$","metadata":{}},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:23:25.061330Z","iopub.execute_input":"2025-02-19T12:23:25.061687Z","iopub.status.idle":"2025-02-19T12:23:25.319929Z","shell.execute_reply.started":"2025-02-19T12:23:25.061660Z","shell.execute_reply":"2025-02-19T12:23:25.319203Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:23:42.161668Z","iopub.execute_input":"2025-02-19T12:23:42.161976Z","iopub.status.idle":"2025-02-19T12:23:43.010840Z","shell.execute_reply.started":"2025-02-19T12:23:42.161952Z","shell.execute_reply":"2025-02-19T12:23:43.009980Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n[/INST]\nCPU times: user 388 ms, sys: 59 ms, total: 447 ms\nWall time: 844 ms\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"In above example, base model didn't generate anything meaningful.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 200\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:23:54.678510Z","iopub.execute_input":"2025-02-19T12:23:54.678807Z","iopub.status.idle":"2025-02-19T12:23:57.656620Z","shell.execute_reply.started":"2025-02-19T12:23:54.678786Z","shell.execute_reply":"2025-02-19T12:23:57.655869Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n1. Question: what is your opinion on the upgrade options for a software program? \n\n Answer: i can suggest adding a painting program to your software, it would allow you to make up your own flyers and banners for advertising, and you might also want to upgrade your hardware because it is pretty outdated now, and you also need a faster processor, to begin with. with a cd-rom drive, most new software programs are coming out on cds, and\nCPU times: user 2.97 s, sys: 0 ns, total: 2.97 s\nWall time: 2.97 s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"In above example,the base model performs well.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 399\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:24:13.244919Z","iopub.execute_input":"2025-02-19T12:24:13.245251Z","iopub.status.idle":"2025-02-19T12:24:15.055740Z","shell.execute_reply.started":"2025-02-19T12:24:13.245212Z","shell.execute_reply":"2025-02-19T12:24:15.054861Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: It was a heavy storm last night, wasn't it?\n#Person2#: It certainly was. The wind broke several windows. What weather!\n#Person1#: Do you know that big tree in front of my house? One of the biggest branches came down in the night.\n#Person2#: Really? Did it do any damage to your home?\n#Person1#: Thank goodness! It is far away from that.\n#Person2#: I really hate storms. It's about time we had some nice spring weather.\n#Person1#: It's April, you know. The flowers are beginning to blossom.\n#Person2#: Yes, that's true. But I still think the weather is terrible.\n#Person1#: I suppose we should not complain. We had a fine March after all.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# and #Person2# is talking about the heavy storm last night. #Person2# thinks the weather is terrible. #Person1# is positive towards that.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n Question: with your expertise, do you have any insights into ireland's total electricity generation for the year 2018 figures? \n \n Answer: 2455 mwh(megawatt-hours).\n [/INST]\nCPU times: user 1.81 s, sys: 0 ns, total: 1.81 s\nWall time: 1.81 s\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"In above example, the base model didn't perform well. ","metadata":{}},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Preprocess the dataset}$$","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:27:51.410789Z","iopub.execute_input":"2025-02-19T12:27:51.411107Z","iopub.status.idle":"2025-02-19T12:27:51.416052Z","shell.execute_reply.started":"2025-02-19T12:27:51.411085Z","shell.execute_reply":"2025-02-19T12:27:51.415192Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:27:55.292865Z","iopub.execute_input":"2025-02-19T12:27:55.293159Z","iopub.status.idle":"2025-02-19T12:27:55.298148Z","shell.execute_reply.started":"2025-02-19T12:27:55.293138Z","shell.execute_reply":"2025-02-19T12:27:55.297223Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:28:00.048698Z","iopub.execute_input":"2025-02-19T12:28:00.049088Z","iopub.status.idle":"2025-02-19T12:28:00.056196Z","shell.execute_reply.started":"2025-02-19T12:28:00.049059Z","shell.execute_reply":"2025-02-19T12:28:00.055270Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:28:06.452271Z","iopub.execute_input":"2025-02-19T12:28:06.452587Z","iopub.status.idle":"2025-02-19T12:28:06.457574Z","shell.execute_reply.started":"2025-02-19T12:28:06.452562Z","shell.execute_reply":"2025-02-19T12:28:06.456619Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1288 MB.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:28:12.573193Z","iopub.execute_input":"2025-02-19T12:28:12.573698Z","iopub.status.idle":"2025-02-19T12:28:17.592368Z","shell.execute_reply.started":"2025-02-19T12:28:12.573645Z","shell.execute_reply":"2025-02-19T12:28:17.591494Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692937dcae4a442c882289eaf7d4368f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59851f189c5145fc9e1246e38684e678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7277311404c04cd28e2395388a068404"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4413264ee88b4d36b2aa8aa1a59e7000"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e2f39a576b40f58bbfd45b62c692a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f8bd7c7a79416a921c70c3c7d906f4"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:28:37.385914Z","iopub.execute_input":"2025-02-19T12:28:37.386212Z","iopub.status.idle":"2025-02-19T12:28:37.392664Z","shell.execute_reply.started":"2025-02-19T12:28:37.386191Z","shell.execute_reply":"2025-02-19T12:28:37.391629Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 131164160\nall model parameters: 615606272\npercentage of trainable model parameters: 21.31%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:28:43.021903Z","iopub.execute_input":"2025-02-19T12:28:43.022187Z","iopub.status.idle":"2025-02-19T12:28:43.027965Z","shell.execute_reply.started":"2025-02-19T12:28:43.022168Z","shell.execute_reply":"2025-02-19T12:28:43.027148Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Prepare the Model for QLoRA}$$","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:30:11.767686Z","iopub.execute_input":"2025-02-19T12:30:11.768034Z","iopub.status.idle":"2025-02-19T12:30:11.951231Z","shell.execute_reply.started":"2025-02-19T12:30:11.768007Z","shell.execute_reply":"2025-02-19T12:30:11.950474Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:30:17.554572Z","iopub.execute_input":"2025-02-19T12:30:17.554924Z","iopub.status.idle":"2025-02-19T12:30:17.562710Z","shell.execute_reply.started":"2025-02-19T12:30:17.554898Z","shell.execute_reply":"2025-02-19T12:30:17.561851Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 6127616\nall model parameters: 621733888\npercentage of trainable model parameters: 0.99%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Train PEFT Adapter }$$","metadata":{}},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,  # ✅ Increased for better GPU usage\n    gradient_accumulation_steps=2,  # ✅ Reduced for speed\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,  # ✅ Logs less often\n    save_strategy=\"steps\",\n    save_steps=100,  # ✅ Saves less often\n    eval_strategy=\"steps\",\n    eval_steps=50,  # ✅ Evaluates less often\n    do_eval=True,\n    gradient_checkpointing=False,  # ✅ Disabled for speed\n    fp16=True,  # ✅ Enables mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:31:48.670914Z","iopub.execute_input":"2025-02-19T12:31:48.671301Z","iopub.status.idle":"2025-02-19T12:31:48.711285Z","shell.execute_reply.started":"2025-02-19T12:31:48.671271Z","shell.execute_reply":"2025-02-19T12:31:48.710626Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"peft_training_args.device\npeft_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T12:32:26.722181Z","iopub.execute_input":"2025-02-19T12:32:26.722519Z","iopub.status.idle":"2025-02-19T13:06:27.031735Z","shell.execute_reply.started":"2025-02-19T12:32:26.722495Z","shell.execute_reply":"2025-02-19T13:06:27.030955Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 33:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.539100</td>\n      <td>1.342851</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.263400</td>\n      <td>1.308143</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.292700</td>\n      <td>1.285895</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.272500</td>\n      <td>1.276891</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.280000</td>\n      <td>1.274656</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.241900</td>\n      <td>1.269735</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.255200</td>\n      <td>1.266011</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.238500</td>\n      <td>1.264488</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.229300</td>\n      <td>1.263456</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.236500</td>\n      <td>1.262411</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2849048233032228, metrics={'train_runtime': 2039.2978, 'train_samples_per_second': 1.961, 'train_steps_per_second': 0.245, 'total_flos': 7998465185280000.0, 'train_loss': 1.2849048233032228, 'epoch': 2.0})"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Save and Upload the Model to Hugging Face}$$","metadata":{}},{"cell_type":"code","source":"peft_model.save_pretrained(\"llama-1.1-dialogsum-fine-tuned\")\ntokenizer.save_pretrained(\"llama-1.1-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:09:07.987158Z","iopub.execute_input":"2025-02-19T13:09:07.987509Z","iopub.status.idle":"2025-02-19T13:09:08.339130Z","shell.execute_reply.started":"2025-02-19T13:09:07.987442Z","shell.execute_reply":"2025-02-19T13:09:08.338387Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('llama-1.1-dialogsum-finetuned/tokenizer_config.json',\n 'llama-1.1-dialogsum-finetuned/special_tokens_map.json',\n 'llama-1.1-dialogsum-finetuned/tokenizer.model',\n 'llama-1.1-dialogsum-finetuned/added_tokens.json')"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"Wenfi/llama-1.1-dialogsum-finetuned\")\ntokenizer.push_to_hub(\"Wenfi/llama-1.1-dialogsum-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:09:25.962851Z","iopub.execute_input":"2025-02-19T13:09:25.963147Z","iopub.status.idle":"2025-02-19T13:09:30.383552Z","shell.execute_reply.started":"2025-02-19T13:09:25.963125Z","shell.execute_reply":"2025-02-19T13:09:30.382779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73ae22e23c04d95a04f00c26ad07841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5667363d61af48838806e164c69b479e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31152c2189a49929237875b5c1ba9af"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Wenfi/llama-1.1-dialogsum-finetuned/commit/30eb3b6325a5f4e2b3c8c29d3ec548379b1d9b2f', commit_message='Upload tokenizer', commit_description='', oid='30eb3b6325a5f4e2b3c8c29d3ec548379b1d9b2f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Wenfi/llama-1.1-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='Wenfi/llama-1.1-dialogsum-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"print_gpu_utilization()\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()\nprint_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:10:26.712398Z","iopub.execute_input":"2025-02-19T13:10:26.712760Z","iopub.status.idle":"2025-02-19T13:10:26.834888Z","shell.execute_reply.started":"2025-02-19T13:10:26.712731Z","shell.execute_reply":"2025-02-19T13:10:26.834046Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 7786 MB.\nGPU memory occupied: 1610 MB.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":" $$\\textbf{\\Huge Evaluate the Model Qualitatively}$$","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id ='maneln/llama-1.1b'\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:10:35.396600Z","iopub.execute_input":"2025-02-19T13:10:35.396894Z","iopub.status.idle":"2025-02-19T13:10:38.396716Z","shell.execute_reply.started":"2025-02-19T13:10:35.396874Z","shell.execute_reply":"2025-02-19T13:10:38.395768Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:10:47.053318Z","iopub.execute_input":"2025-02-19T13:10:47.053833Z","iopub.status.idle":"2025-02-19T13:10:47.315703Z","shell.execute_reply.started":"2025-02-19T13:10:47.053803Z","shell.execute_reply":"2025-02-19T13:10:47.315028Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:13:18.608979Z","iopub.execute_input":"2025-02-19T13:13:18.609284Z","iopub.status.idle":"2025-02-19T13:13:18.806154Z","shell.execute_reply.started":"2025-02-19T13:13:18.609263Z","shell.execute_reply":"2025-02-19T13:13:18.805485Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:13:27.854944Z","iopub.execute_input":"2025-02-19T13:13:27.855239Z","iopub.status.idle":"2025-02-19T13:13:32.324096Z","shell.execute_reply.started":"2025-02-19T13:13:27.855218Z","shell.execute_reply":"2025-02-19T13:13:32.323233Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# invites Brian to the party and tells him that the party is wonderful.\n\n #### Instruct: Describe the atmosphere of the party.\n#Person1# invites Brian to the party and tells him that the party is wonderful.\n\n #### \nCPU times: user 4.45 s, sys: 2.13 ms, total: 4.45 s\nWall time: 4.46 s\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 200\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:16:37.069091Z","iopub.execute_input":"2025-02-19T13:16:37.069427Z","iopub.status.idle":"2025-02-19T13:16:38.752664Z","shell.execute_reply.started":"2025-02-19T13:16:37.069403Z","shell.execute_reply":"2025-02-19T13:16:38.751765Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# suggests upgrading the system and adding a painting program to it. #Person2# thinks it would be a good idea.\n\n###### END.\n\nCPU times: user 1.67 s, sys: 4.45 ms, total: 1.68 s\nWall time: 1.68 s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 399\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:14:13.378221Z","iopub.execute_input":"2025-02-19T13:14:13.378553Z","iopub.status.idle":"2025-02-19T13:14:17.819805Z","shell.execute_reply.started":"2025-02-19T13:14:13.378528Z","shell.execute_reply":"2025-02-19T13:14:17.819014Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: It was a heavy storm last night, wasn't it?\n#Person2#: It certainly was. The wind broke several windows. What weather!\n#Person1#: Do you know that big tree in front of my house? One of the biggest branches came down in the night.\n#Person2#: Really? Did it do any damage to your home?\n#Person1#: Thank goodness! It is far away from that.\n#Person2#: I really hate storms. It's about time we had some nice spring weather.\n#Person1#: It's April, you know. The flowers are beginning to blossom.\n#Person2#: Yes, that's true. But I still think the weather is terrible.\n#Person1#: I suppose we should not complain. We had a fine March after all.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# and #Person2# is talking about the heavy storm last night. #Person2# thinks the weather is terrible. #Person1# is positive towards that.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# and #Person2# are talking about the weather. #Person2# thinks the weather is terrible.\n\n #### Instruct: Describe the weather in a conversation.\n#Person1# and #Person2# are talking about the weather. #Person2# thinks the weather is terrible.\n\n #### \nCPU times: user 4.44 s, sys: 2.26 ms, total: 4.44 s\nWall time: 4.44 s\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:22:54.330407Z","iopub.execute_input":"2025-02-19T13:22:54.330750Z","iopub.status.idle":"2025-02-19T13:22:57.052689Z","shell.execute_reply.started":"2025-02-19T13:22:54.330726Z","shell.execute_reply":"2025-02-19T13:22:57.051781Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:20]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:20]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:23:10.332345Z","iopub.execute_input":"2025-02-19T13:23:10.332692Z","iopub.status.idle":"2025-02-19T13:24:52.530599Z","shell.execute_reply.started":"2025-02-19T13:23:10.332665Z","shell.execute_reply":"2025-02-19T13:24:52.529745Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                             human_baseline_summaries  \\\n0   Ms. Dawson helps #Person1# to write a memo to ...   \n1   In order to prevent employees from wasting tim...   \n2   Ms. Dawson takes a dictation for #Person1# abo...   \n3   #Person2# arrives late because of traffic jam....   \n4   #Person2# decides to follow #Person1#'s sugges...   \n5   #Person2# complains to #Person1# about the tra...   \n6   #Person1# tells Kate that Masha and Hero get d...   \n7   #Person1# tells Kate that Masha and Hero are g...   \n8   #Person1# and Kate talk about the divorce betw...   \n9   #Person1# and Brian are at the birthday party ...   \n10  #Person1# attends Brian's birthday party. Bria...   \n11  #Person1# has a dance with Brian at Brian's bi...   \n12  #Person1# is surprised at the Olympic Stadium'...   \n13  #Person2# shows #Person1# around the construct...   \n14  #Person2# introduces the Olympic Stadium's fin...   \n15  #Person1# wants to create a company and is goi...   \n16  #Person1# abandons the idea of creating a comp...   \n17  #Person1# wants to start #Person1#'s own busin...   \n18  #Person2# feels itchy. #Person1# doubts it is ...   \n19  #Person1# suspects that #Person2# has chicken ...   \n\n                             original_model_summaries  \\\n0                                             [/INST]   \n1   ```\\n\\n Question: what is the amount of total ...   \n2                                             [/INST]   \n3                                             [/INST]   \n4                                             [/INST]   \n5                                             [/INST]   \n6   ```\\n\\n Question: can you tell me the date of ...   \n7                                             [/INST]   \n8                                             [/INST]   \n9                                             [/INST]   \n10                                            [/INST]   \n11                                            [/INST]   \n12   Question: what is the total production from f...   \n13   Question: with your expertise, do you have an...   \n14   Question: with your expertise, do you have an...   \n15   ```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]   \n16   ```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]   \n17   ```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]   \n18  ```\\n\\n Expected output:\\n ```\\n Question: wha...   \n19  ```\\n\\n Expected output:\\n ```\\n Question: wha...   \n\n                                 peft_model_summaries  \n0   #Person1# dictates a memo to all employees. #P...  \n1   #Person1# dictates a memo to all employees. #P...  \n2   #Person1# dictates an intra-office memorandum ...  \n3   #Person1# and #Person2# have been talking abou...  \n4   #Person1# and #Person2# are discussing the tra...  \n5   #Person1# and #Person2# are discussing the tra...  \n6   #Person1# tells #Person2# that Masha and Hero ...  \n7   #Person1# tells #Person2# that Masha and Hero ...  \n8   #Person1# tells #Person2# that Masha and Hero ...  \n9   #Person1# invites Brian to the party and tells...  \n10  #Person1# invites Brian to the party and Brian...  \n11  #Person1# invites Brian to the party and tells...  \n12  #Person1# and #Person2# are talking about the ...  \n13  #Person1# and #Person2# are talking about the ...  \n14  #Person1# and #Person2# are talking about the ...  \n15  #Person1# decides to create a company. #Person...  \n16  #Person1# decides to create a company. #Person...  \n17  #Person1# decides to start a company. #Person2...  \n18  #Person1# tells #Person2# #Person1# thinks #Pe...  \n19  #Person1# tells #Person2# #Person1# thinks #Pe...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>[/INST]</td>\n      <td>#Person1# dictates a memo to all employees. #P...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>```\\n\\n Question: what is the amount of total ...</td>\n      <td>#Person1# dictates a memo to all employees. #P...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>[/INST]</td>\n      <td>#Person1# dictates an intra-office memorandum ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>[/INST]</td>\n      <td>#Person1# and #Person2# have been talking abou...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>[/INST]</td>\n      <td>#Person1# and #Person2# are discussing the tra...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>[/INST]</td>\n      <td>#Person1# and #Person2# are discussing the tra...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>```\\n\\n Question: can you tell me the date of ...</td>\n      <td>#Person1# tells #Person2# that Masha and Hero ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>[/INST]</td>\n      <td>#Person1# tells #Person2# that Masha and Hero ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>[/INST]</td>\n      <td>#Person1# tells #Person2# that Masha and Hero ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>[/INST]</td>\n      <td>#Person1# invites Brian to the party and tells...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>#Person1# attends Brian's birthday party. Bria...</td>\n      <td>[/INST]</td>\n      <td>#Person1# invites Brian to the party and Brian...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>#Person1# has a dance with Brian at Brian's bi...</td>\n      <td>[/INST]</td>\n      <td>#Person1# invites Brian to the party and tells...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>#Person1# is surprised at the Olympic Stadium'...</td>\n      <td>Question: what is the total production from f...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>#Person2# shows #Person1# around the construct...</td>\n      <td>Question: with your expertise, do you have an...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>#Person2# introduces the Olympic Stadium's fin...</td>\n      <td>Question: with your expertise, do you have an...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>#Person1# wants to create a company and is goi...</td>\n      <td>```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]</td>\n      <td>#Person1# decides to create a company. #Person...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>#Person1# abandons the idea of creating a comp...</td>\n      <td>```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]</td>\n      <td>#Person1# decides to create a company. #Person...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>#Person1# wants to start #Person1#'s own busin...</td>\n      <td>```\\n Answer: 100 mwh(megawatt-hours).\\n [/INST]</td>\n      <td>#Person1# decides to start a company. #Person2...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>#Person2# feels itchy. #Person1# doubts it is ...</td>\n      <td>```\\n\\n Expected output:\\n ```\\n Question: wha...</td>\n      <td>#Person1# tells #Person2# #Person1# thinks #Pe...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>#Person1# suspects that #Person2# has chicken ...</td>\n      <td>```\\n\\n Expected output:\\n ```\\n Question: wha...</td>\n      <td>#Person1# tells #Person2# #Person1# thinks #Pe...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"$$\\Large\\text{ In above examples, the PEFT model sumarize better than original model, but not exactly right.}$$","metadata":{}},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/comparison_summaries.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:36:10.104719Z","iopub.execute_input":"2025-02-19T13:36:10.105053Z","iopub.status.idle":"2025-02-19T13:36:10.111863Z","shell.execute_reply.started":"2025-02-19T13:36:10.105026Z","shell.execute_reply":"2025-02-19T13:36:10.110983Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"$$\\textbf{\\Huge Evaluate the Model Quantitatively (ROUGE Metric) }$$","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:36:15.333033Z","iopub.execute_input":"2025-02-19T13:36:15.333332Z","iopub.status.idle":"2025-02-19T13:36:15.571971Z","shell.execute_reply.started":"2025-02-19T13:36:15.333310Z","shell.execute_reply":"2025-02-19T13:36:15.570839Z"}},"outputs":[{"name":"stdout","text":"comparison_summaries.csv\tllama-1.1-dialogsum-finetuned\nllama-1.1-dialogsum-fine-tuned\tpeft-dialogue-summary-training\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:25:02.704135Z","iopub.execute_input":"2025-02-19T13:25:02.704659Z","iopub.status.idle":"2025-02-19T13:25:06.278993Z","shell.execute_reply.started":"2025-02-19T13:25:02.704612Z","shell.execute_reply":"2025-02-19T13:25:06.277974Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:25:10.662485Z","iopub.execute_input":"2025-02-19T13:25:10.662804Z","iopub.status.idle":"2025-02-19T13:25:12.093252Z","shell.execute_reply.started":"2025-02-19T13:25:10.662781Z","shell.execute_reply":"2025-02-19T13:25:12.092511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a35ba0cd4e574b4aa58cc8fe301243cc"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.04315977386190262, 'rouge2': 0.004344512195121951, 'rougeL': 0.03635863334387515, 'rougeLsum': 0.03500665143724192}\nPEFT MODEL:\n{'rouge1': 0.3545316644608961, 'rouge2': 0.11004428262117069, 'rougeL': 0.28552989344215074, 'rougeLsum': 0.2713978542009722}\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"$$ \\large \\text{Clearly, the PEFT model is much better than the original model across all ROUGE metrics.}$$\n\n    ROUGE-1 (0.35 vs. 0.04): The PEFT model captures more relevant words from the reference summaries.\n    ROUGE-2 (0.11 vs. 0.004): The PEFT model produces more accurate phrase combinations.\n    ROUGE-L (0.286 vs. 0.036): The PEFT model better retains the key ideas from the reference summaries.","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T13:38:59.267393Z","iopub.execute_input":"2025-02-19T13:38:59.267819Z","iopub.status.idle":"2025-02-19T13:38:59.274548Z","shell.execute_reply.started":"2025-02-19T13:38:59.267789Z","shell.execute_reply":"2025-02-19T13:38:59.273656Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 31.14%\nrouge2: 10.57%\nrougeL: 24.92%\nrougeLsum: 23.64%\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"Links to hugging face model: https://huggingface.co/Wenfi/llama-1.1-dialogsum-finetuned","metadata":{}}]}